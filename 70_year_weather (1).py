# -*- coding: utf-8 -*-
"""70 year weather.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EVlQWKBXDnAbMyRL0LJMS-PcH-1YgirR
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/weather data 70 years.csv')

# Display the first few rows of the dataset
print(df.head())

# Check for missing values
print(df.isnull().sum())

# Check data types
print(df.dtypes)

# Check for duplicates
print(df.duplicated().sum())

# Fill missing values with the mean (for numerical columns)
df['Rain'].fillna(df['Rain'].mean(), inplace=True)
df['Temp Max'].fillna(df['Temp Max'].mean(), inplace=True)
df['Temp Min'].fillna(df['Temp Min'].mean(), inplace=True)

# Alternatively, drop rows with missing values
df.dropna(inplace=True)

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Drop duplicates
df.drop_duplicates(inplace=True)

# Extract year, month, and day from the 'Date' column
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day

from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Scale the numerical columns
df[['Rain', 'Temp Max', 'Temp Min']] = scaler.fit_transform(df[['Rain', 'Temp Max', 'Temp Min']])

# Save the refined dataset to a new CSV file
df.to_csv('refined_weather_data.csv', index=False)

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the distribution of rainfall
sns.histplot(df['Rain'], kde=True)
plt.title('Distribution of Rainfall')
plt.show()

# Plot the correlation matrix
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Define features and target variable
X = df[['Temp Max', 'Temp Min', 'Year', 'Month', 'Day']]
y = df['Rain']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print(f'Mean Squared Error (MSE): {mse}')
print(f'Mean Absolute Error (MAE): {mae}')
print(f'R-squared (R²): {r2}')
print(f'Root Mean Squared Error (RMSE): {rmse}')

"""# single code of above"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import KNNImputer
import shap
import joblib

# Load the dataset
df = pd.read_csv('/content/cleaned_weather_data.csv')

# Display basic information about the dataset
print("Initial Data Overview:")
print(df.head())
print("\nData Info:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())

# Data Cleaning
# Drop duplicates
df.drop_duplicates(inplace=True)

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Extract year, month, and day from the 'Date' column
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day

# Handle missing values using KNN Imputer
imputer = KNNImputer(n_neighbors=5)
df[['Rain', 'Temp Max', 'Temp Min']] = imputer.fit_transform(df[['Rain', 'Temp Max', 'Temp Min']])

# Outlier Detection using IQR
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])
    return df

for col in ['Rain', 'Temp Max', 'Temp Min']:
    df = handle_outliers(df, col)

# Feature Engineering
# Add additional features like season, day of year, etc.
df['DayOfYear'] = df['Date'].dt.dayofyear
df['Season'] = df['Month'] % 12 // 3 + 1  # 1: Winter, 2: Spring, 3: Summer, 4: Fall

# Create lag features
df['Rain_Lag1'] = df['Rain'].shift(1)
df['Temp_Max_Lag1'] = df['Temp Max'].shift(1)
df['Temp_Min_Lag1'] = df['Temp Min'].shift(1)

# Create rolling statistics
df['Rain_Rolling_Mean'] = df['Rain'].rolling(window=7).mean()
df['Temp_Max_Rolling_Mean'] = df['Temp Max'].rolling(window=7).mean()
df['Temp_Min_Rolling_Mean'] = df['Temp Min'].rolling(window=7).mean()

# Drop rows with NaN values created by lag and rolling features
df.dropna(inplace=True)

# Data Visualization
# Plot the distribution of rainfall
plt.figure(figsize=(10, 6))
sns.histplot(df['Rain'], kde=True, bins=30)
plt.title('Distribution of Rainfall')
plt.show()

# Plot temperature trends over time
plt.figure(figsize=(12, 6))
plt.plot(df['Date'], df['Temp Max'], label='Max Temp', alpha=0.7)
plt.plot(df['Date'], df['Temp Min'], label='Min Temp', alpha=0.7)
plt.xlabel('Date')
plt.ylabel('Temperature')
plt.title('Temperature Trends Over Time')
plt.legend()
plt.show()

# Plot the correlation matrix
plt.figure(figsize=(10, 8))
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()

# Data Scaling
# Initialize the scaler
scaler = MinMaxScaler()

# Scale the numerical columns
numerical_cols = ['Rain', 'Temp Max', 'Temp Min']
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Save the refined dataset to a new CSV file
df.to_csv('refined_weather_data.csv', index=False)

# Model Training
# Define features and target variable
features = ['Temp Max', 'Temp Min', 'Year', 'Month', 'Day', 'DayOfYear', 'Season', 'Rain_Lag1', 'Temp_Max_Lag1', 'Temp_Min_Lag1', 'Rain_Rolling_Mean', 'Temp_Max_Rolling_Mean', 'Temp_Min_Rolling_Mean']
X = df[features]
y = df['Rain']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hyperparameter Tuning with GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the GridSearchCV
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions with the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print("\nBest Model Evaluation:")
print(f'Mean Squared Error (MSE): {mse}')
print(f'Mean Absolute Error (MAE): {mae}')
print(f'R-squared (R²): {r2}')
print(f'Root Mean Squared Error (RMSE): {rmse}')

# Feature Importance
feature_importance = pd.DataFrame({
    'Feature': features,
    'Importance': best_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('Feature Importance')
plt.show()

# SHAP Values for Model Interpretability
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test)

# Plot SHAP summary plot
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Save the best model to a file
joblib.dump(best_model, 'best_random_forest_model.pkl')

# Plot predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Rainfall')
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer

# Load data
df = pd.read_csv('/content/cleaned_weather_data.csv')

# Data Cleaning
df.drop_duplicates(inplace=True)
df['Date'] = pd.to_datetime(df['Date'])
df = df[df['Rain'] != 0]

# Impute missing values with mean
imputer = SimpleImputer(strategy='mean')
df[['Rain', 'Temp Max', 'Temp Min']] = imputer.fit_transform(df[['Rain', 'Temp Max', 'Temp Min']])

# Feature Engineering
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfYear'] = df['Date'].dt.dayofyear
df['Season'] = df['Month'] % 12 // 3 + 1

# Create lag features
df['Rain_Lag1'] = df['Rain'].shift(1)
df['Temp_Max_Lag1'] = df['Temp Max'].shift(1)
df['Temp_Min_Lag1'] = df['Temp Min'].shift(1)

# Drop rows with NaN values
df.dropna(inplace=True)

# Data Scaling
scaler = MinMaxScaler()
features = ['Temp Max', 'Temp Min', 'Year', 'Month', 'Day', 'DayOfYear', 'Season', 'Rain_Lag1', 'Temp_Max_Lag1', 'Temp_Min_Lag1']
df[features] = scaler.fit_transform(df[features])

# Model Training
X = df[features]
y = df['Rain']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hyperparameter Tuning
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_

# Prediction function
def predict_rainfall(temp_max, temp_min):
    input_data = np.array([[temp_max, temp_min, 2023, 10, 15, 288, 2, 0, 0, 0, 0, 0, 0]])  # Example values for other features
    input_data_scaled = scaler.transform(input_data)  # Scale the input data
    prediction = best_model.predict(input_data_scaled)
    return prediction[0]

# Manual input for temperature
temp_max = float(input("Enter the maximum temperature: "))
temp_min = float(input("Enter the minimum temperature: "))
predicted_rainfall = predict_rainfall(temp_max, temp_min)

print(f"Predicted rainfall based on the input temperature: {predicted_rainfall} mm")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import KNNImputer
import shap
import joblib
# Load the dataset
df = pd.read_csv('/content/weather data 70 years.csv')

# Display basic information about the dataset
print("Initial Data Overview:")
print(df.head())
print("\nData Info:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())

# Data Cleaning
# Drop duplicates
df.drop_duplicates(inplace=True)

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Remove rows where Rain is 0
df = df[df['Rain'] != 0].copy()  # Use .copy() to avoid SettingWithCopyWarning

# Extract year, month, and day from the 'Date' column
df.loc[:, 'Year'] = df['Date'].dt.year
df.loc[:, 'Month'] = df['Date'].dt.month
df.loc[:, 'Day'] = df['Date'].dt.day

# Handle missing values using KNN Imputer
imputer = KNNImputer(n_neighbors=5)
df[['Rain', 'Temp Max', 'Temp Min']] = imputer.fit_transform(df[['Rain', 'Temp Max', 'Temp Min']])

# Outlier Detection using IQR
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df.loc[:, column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df.loc[:, column] = np.where(df[column] > upper_bound, upper_bound, df[column])
    return df

for col in ['Rain', 'Temp Max', 'Temp Min']:
    df = handle_outliers(df, col)

# Feature Engineering
# Add additional features like season, day of year, etc.
df.loc[:, 'DayOfYear'] = df['Date'].dt.dayofyear
df.loc[:, 'Season'] = df['Month'] % 12 // 3 + 1  # 1: Winter, 2: Spring, 3: Summer, 4: Fall

# Create lag features
df.loc[:, 'Rain_Lag1'] = df['Rain'].shift(1)
df.loc[:, 'Temp_Max_Lag1'] = df['Temp Max'].shift(1)
df.loc[:, 'Temp_Min_Lag1'] = df['Temp Min'].shift(1)

# Create rolling statistics
df.loc[:, 'Rain_Rolling_Mean'] = df['Rain'].rolling(window=7).mean()
df.loc[:, 'Temp_Max_Rolling_Mean'] = df['Temp Max'].rolling(window=7).mean()
df.loc[:, 'Temp_Min_Rolling_Mean'] = df['Temp Min'].rolling(window=7).mean()

# Drop rows with NaN values created by lag and rolling features
df.dropna(inplace=True)

# Data Scaling
# Initialize the scaler
scaler = MinMaxScaler()

# Scale all the features used in the model
features = ['Temp Max', 'Temp Min', 'Year', 'Month', 'Day', 'DayOfYear', 'Season', 'Rain_Lag1', 'Temp_Max_Lag1', 'Temp_Min_Lag1', 'Rain_Rolling_Mean', 'Temp_Max_Rolling_Mean', 'Temp_Min_Rolling_Mean']
df[features] = scaler.fit_transform(df[features])

# Model Training
# Define features and target variable
X = df[features]
y = df['Rain']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hyperparameter Tuning with GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the GridSearchCV
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Function to predict rainfall based on user input
def predict_rainfall(temp_max, temp_min):
    # Prepare the input data
    input_data = np.array([[temp_max, temp_min, 2023, 10, 15, 288, 2, 0, 0, 0, 0, 0, 0]])  # Example values for other features
    input_data_scaled = scaler.transform(input_data)  # Scale the input data

    # Make prediction
    prediction = best_model.predict(input_data_scaled)
    return prediction[0]

# Manual input for temperature and humidity
temp_max = float(input("Enter the maximum temperature: "))
temp_min = float(input("Enter the minimum temperature: "))
predicted_rainfall = predict_rainfall(temp_max, temp_min)

print(f"Predicted rainfall based on the input temperature: {predicted_rainfall} mm")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer

# Load data
df = pd.read_csv('/content/cleaned_weather_data.csv')

# Display basic information about the dataset
print("Initial Data Overview:")
print(df.head())
print("\nData Info:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())

# Data Cleaning
# Drop duplicates
df.drop_duplicates(inplace=True)

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Remove rows where Rain is 0
df = df[df['Rain'] != 0]

# Impute missing values with mean
imputer = SimpleImputer(strategy='mean')
df[['Rain', 'Temp Max', 'Temp Min']] = imputer.fit_transform(df[['Rain', 'Temp Max', 'Temp Min']])

# Feature Engineering
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfYear'] = df['Date'].dt.dayofyear
df['Season'] = df['Month'] % 12 // 3 + 1  # 1: Winter, 2: Spring, 3: Summer, 4: Fall

# Create lag features
df['Rain_Lag1'] = df['Rain'].shift(1)
df['Temp_Max_Lag1'] = df['Temp Max'].shift(1)
df['Temp_Min_Lag1'] = df['Temp Min'].shift(1)

# Drop rows with NaN values created by lag features
df.dropna(inplace=True)

# Data Scaling
scaler = MinMaxScaler()
features = ['Temp Max', 'Temp Min', 'Year', 'Month', 'Day', 'DayOfYear', 'Season', 'Rain_Lag1', 'Temp_Max_Lag1', 'Temp_Min_Lag1']
df[features] = scaler.fit_transform(df[features])

# Model Training
X = df[features]
y = df['Rain']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hyperparameter Tuning
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Function to predict rainfall based on user input
def predict_rainfall(temp_max, temp_min):
    # Prepare the input data with only the features used in the model
    input_data = np.array([[temp_max, temp_min, 2023, 10, 15, 288, 2, 0, 0, 0]])  # Example values for lag features
    input_data_scaled = scaler.transform(input_data)  # Scale the input data

    # Make prediction
    prediction = best_model.predict(input_data_scaled)
    return prediction[0]

# Manual input for temperature
temp_max = float(input("Enter the maximum temperature: "))
temp_min = float(input("Enter the minimum temperature: "))
predicted_rainfall = predict_rainfall(temp_max, temp_min)

print(f"Predicted rainfall based on the input temperature: {predicted_rainfall} mm")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, r2_score

# Load data
df = pd.read_csv('/content/refined_weather_data.csv')

# Display basic information about the dataset
print("Initial Data Overview:")
print(df.head())
print("\nData Info:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())

# Data Cleaning
# Drop duplicates
df.drop_duplicates(inplace=True)

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Remove rows where Rain is 0
df = df[df['Rain'] != 0]

# Impute missing values with mean
imputer = SimpleImputer(strategy='mean')
df[['Rain', 'Temp Max', 'Temp Min']] = imputer.fit_transform(df[['Rain', 'Temp Max', 'Temp Min']])

# Feature Engineering
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfYear'] = df['Date'].dt.dayofyear
df['Season'] = df['Month'] % 12 // 3 + 1  # 1: Winter, 2: Spring, 3: Summer, 4: Fall

# Create lag features
df['Rain_Lag1'] = df['Rain'].shift(1)
df['Temp_Max_Lag1'] = df['Temp Max'].shift(1)
df['Temp_Min_Lag1'] = df['Temp Min'].shift(1)

# Drop rows with NaN values created by lag features
df.dropna(inplace=True)

# Data Scaling
scaler = MinMaxScaler()
features = ['Temp Max', 'Temp Min', 'Year', 'Month', 'Day', 'DayOfYear', 'Season', 'Rain_Lag1', 'Temp_Max_Lag1', 'Temp_Min_Lag1']
df[features] = scaler.fit_transform(df[features])

# Model Training
X = df[features]
y = df['Rain']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hyperparameter Tuning
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions for the test set
y_pred_test = best_model.predict(X_test)

# Calculate Mean Squared Error and R-squared
mse = mean_squared_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")

# Function to predict rainfall based on user input
def predict_rainfall(temp_max, temp_min):
    # Prepare the input data with only the features used in the model
    input_data = np.array([[temp_max, temp_min, 2023, 10, 15, 288, 2, 0, 0, 0]])  # Example values for lag features
    input_data_scaled = scaler.transform(input_data)  # Scale the input data

    # Make prediction
    prediction = best_model.predict(input_data_scaled)
    return prediction[0]

# Manual input for temperature
temp_max = float(input("Enter the maximum temperature: "))
temp_min = float(input("Enter the minimum temperature: "))
predicted_rainfall = predict_rainfall(temp_max, temp_min)

print(f"Predicted rainfall based on the input temperature: {predicted_rainfall:.2f} mm")

import dht
from machine import Pin, ADC
import time

DHTPIN = 4
MOISTURE_PIN_AO = 36
DRY_VALUE = 3775
WET_VALUE = 1350

dht_sensor = dht.DHT22(Pin(DHTPIN))
moisture_sensor_ao = ADC(Pin(MOISTURE_PIN_AO))
moisture_sensor_ao.atten(ADC.ATTN_11DB)

def map_percentage(x, dry_val, wet_val):
    if wet_val > dry_val:
        wet_val, dry_val = dry_val, wet_val
    if x > dry_val:
        return 0.0
    elif x < wet_val:
        return 100.0
    else:
        return 100.0 - (x - wet_val) * 100.0 / (dry_val - wet_val)

while True:
    try:
        dht_sensor.measure()
        h = dht_sensor.humidity()
        t = int(dht_sensor.temperature())
        m = moisture_sensor_ao.read()
        p = map_percentage(m, DRY_VALUE, WET_VALUE)
        print("T: {} °C, H: {:.2f} %, M: {}, P: {:.2f} %".format(t, h, m, p))
        time.sleep(2)
    except Exception as e:
        print("Error: {}".format(e))
        time.sleep(2)